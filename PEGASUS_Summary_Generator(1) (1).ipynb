{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mk_ak4jJjsQ",
        "outputId": "952abc5d-81db-4e0d-9973-9295d87e2f0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import emoji\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "reviews = pd.read_csv(\"/content/prompts.csv\")\n",
        "reviews = reviews.head(10000)\n",
        "\n",
        "# Preprocessing\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab')  # This line is added to download the necessary data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Slang conversion function\n",
        "def convert_slang(text):\n",
        "    slang_dict = {\n",
        "        'u': 'you', 'r': 'are', 'ur': 'your', 'n': 'and', 'l8r': 'later',\n",
        "        'gr8': 'great', 'b4': 'before', '2nite': 'tonight', 'plz': 'please',\n",
        "        'thx': 'thanks', 'omg': 'oh my god', 'btw': 'by the way',\n",
        "        'bff': 'best friends forever', 'idk': \"I don't know\", 'imo': 'in my opinion', 'lol': 'laugh out loud'\n",
        "    }\n",
        "    words = text.split()\n",
        "    words = [slang_dict[word] if word in slang_dict else word for word in words]\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "CILI_OxBX3nf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Emoji conversion function\n",
        "def convert_emojis(text):\n",
        "    return emoji.demojize(text)\n",
        "\n",
        "# Apply preprocessing\n",
        "# Apply preprocessing\n",
        "# Assuming your text column is named 'act' based on the global variables\n",
        "reviews['Text'] = reviews['act'].str.lower()\n",
        "reviews['Text'] = reviews['Text'].apply(convert_slang)\n",
        "reviews['Text'] = reviews['Text'].apply(convert_emojis)"
      ],
      "metadata": {
        "id": "UXz8ENTPX9F7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization, stopword removal, and punctuation removal\n",
        "tqdm.pandas()\n",
        "reviews['Text'] = reviews['Text'].progress_apply(word_tokenize)\n",
        "reviews['Text'] = reviews['Text'].apply(\n",
        "    lambda tokens: [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
        ")\n",
        "reviews['Text'] = reviews['Text'].apply(lambda tokens: ' '.join(tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_o_h0eUYRQu",
        "outputId": "dd833c34-75c7-407e-8645-109df82f283d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170/170 [00:00<00:00, 7837.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove empty rows\n",
        "reviews = reviews.dropna(subset=['Text'])\n",
        "reviews = reviews[reviews['Text'] != '']"
      ],
      "metadata": {
        "id": "41U8PlvxYtly"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and validation sets\n",
        "train_df, val_df = train_test_split(reviews, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "1vsGNdKpYw-y"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PEGASUS tokenizer and model\n",
        "model_name = \"google/pegasus-xsum\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wLIjeFBY1It",
        "outputId": "c925c72e-5414-407c-85d3-e6308c96e176"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarization function\n",
        "def summarize_text(text, tokenizer, model):\n",
        "    # Extract key sentences for better summarization\n",
        "    key_sentences = sent_tokenize(text)[:2]  # Keep the first two sentences\n",
        "    refined_input = \" \".join(key_sentences)"
      ],
      "metadata": {
        "id": "xw-6nrlXZArc"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarization function\n",
        "def summarize_text(text, tokenizer, model):\n",
        "    # Extract key sentences for better summarization\n",
        "    key_sentences = sent_tokenize(text)[:2]  # Keep the first two sentences\n",
        "    refined_input = \" \".join(key_sentences)\n",
        "    # Tokenize and generate summary\n",
        "    input_ids = tokenizer.encode(\n",
        "        \"summarize: \" + refined_input, return_tensors=\"pt\", max_length=512, truncation=True\n",
        "    )\n",
        "    summary_ids = model.generate(\n",
        "        input_ids, max_length=50, num_beams=6, length_penalty=1.0, early_stopping=True\n",
        "    )\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary"
      ],
      "metadata": {
        "id": "lgDAZdDGZGLM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate summaries\n",
        "generated_summaries = []\n",
        "for text in tqdm(train_df['Text']):\n",
        "    try:\n",
        "        summary = summarize_text(text, tokenizer, model)\n",
        "        generated_summaries.append(summary)\n",
        "    except Exception as e:\n",
        "        print(f\"Error summarizing text: {text[:50]}... | Error: {e}\")\n",
        "        generated_summaries.append(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBbOiBSuZVnR",
        "outputId": "37e63aa5-3cd9-4436-fd72-3ca1a76d52d2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 153/153 [22:16<00:00,  8.73s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add summaries to DataFrame\n",
        "train_df['GeneratedSummary'] = generated_summaries"
      ],
      "metadata": {
        "id": "63ZqpQVtZcir"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to CSV\n",
        "output_path = \"/content/drive/MyDrive/fine_tuned_reviews_with_summaries.csv\"\n",
        "train_df.to_csv(output_path, index=False)\n",
        "print(f\"Summaries saved to {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkUj204sevpR",
        "outputId": "9b3797b3-9b97-4793-92b6-03212eb1fb0f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summaries saved to /content/drive/MyDrive/fine_tuned_reviews_with_summaries.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save fine-tuned model and tokenizer for reuse\n",
        "model.save_pretrained(\"/content/drive/MyDrive/fine_tuned_pegasus_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/fine_tuned_pegasus_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPwsLrleeyIN",
        "outputId": "8126040e-00e6-45df-d642-ebfec4defcf0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'num_beams': 8, 'length_penalty': 0.6}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/fine_tuned_pegasus_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/fine_tuned_pegasus_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/fine_tuned_pegasus_model/spiece.model',\n",
              " '/content/drive/MyDrive/fine_tuned_pegasus_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with an example input\n",
        "input_text = \"If you are looking for the secret ingredient in Robitussin I believe I have found it. I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda. The flavor is very medicinal.\"\n",
        "generated_summary = summarize_text(input_text, tokenizer, model)\n",
        "print(\"Generated Summary:\", generated_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utp-PwlBfERv",
        "outputId": "ba2250c9-4f17-48ed-e98a-690deed4d2e5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: If you are looking for the secret ingredient in Robitussin I believe I have found it.\n"
          ]
        }
      ]
    }
  ]
}